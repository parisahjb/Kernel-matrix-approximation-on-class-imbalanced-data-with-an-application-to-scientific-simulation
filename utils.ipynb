{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5768312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from numpy import linalg as la\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from dppy.finite_dpps import FiniteDPP\n",
    "import seaborn as sns\n",
    "import scipy.io as sio\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "# To plot consistent and pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "mpl.rcParams['font.family'] = 'times new roman'\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.dpi'] = 120\n",
    "mpl.rcParams['savefig.dpi'] = 120\n",
    "import matplotlib\n",
    "matplotlib.axes.Axes.contour\n",
    "matplotlib.pyplot.contour\n",
    "matplotlib.axes.Axes.contourf\n",
    "matplotlib.pyplot.contourf\n",
    "matplotlib.figure.Figure.colorbar\n",
    "matplotlib.pyplot.colorbar\n",
    "matplotlib.axes.Axes.legend\n",
    "matplotlib.pyplot.legend\n",
    "matplotlib.contour.ContourSet\n",
    "matplotlib.contour.ContourSet.legend_elements\n",
    "def landmark_uniform(X, m):\n",
    "    '''Uniform landmark selection'''\n",
    "    # Select m landmarks uniformly at random from the data set X\n",
    "    n = len(X)  # Total number of data points\n",
    "    ind = np.random.choice(n, m, replace=False)  # Randomly choose m indices without replacement\n",
    "    return X[ind]  # Return the selected landmarks\n",
    "\n",
    "def landmark_kmeans(X, m):\n",
    "    '''K-means landmark selection'''\n",
    "    # Apply K-means clustering to the data set X and select the m cluster centroids as landmarks\n",
    "    return KMeans(n_clusters=m, max_iter=100).fit(X).cluster_centers_\n",
    "\n",
    "def landmark_dpp(K_org, X, max_eig, m):\n",
    "    '''DPP landmark selection'''\n",
    "    # Use the Determinantal Point Process (DPP) to select m landmarks based on the correlation kernel\n",
    "    DPP = FiniteDPP(kernel_type='correlation', projection=False, **{'K': K_org / max_eig})\n",
    "    try:\n",
    "        ind = DPP.sample_mcmc_k_dpp(size=m)  # Sample m landmarks using MCMC-based DPP sampling\n",
    "        return X[ind]  # Return the selected landmarks\n",
    "    except:\n",
    "        return np.ones((m, X.shape[1]))  # If an error occurs, return a matrix of ones (fallback option)\n",
    "\n",
    "def landmark_importanceSampling(X, sigma, m, frac, omega):\n",
    "    '''Proposed importance sampling landmark selection\n",
    "       Coarse-to-fine landmark selection strategy:\n",
    "       m/2: uniform\n",
    "       m/2: based on the importance sampling distribution\n",
    "    '''\n",
    "    n = len(X)  # Total number of data points\n",
    "    n_important = np.int(np.floor(frac * n))  # Number of important data points to be selected\n",
    "\n",
    "    # Split the total number of landmarks (m) into two parts: n0 and n1\n",
    "    n0, n1 = np.int(np.floor(m / 2)), np.int(np.ceil(m / 2))\n",
    "\n",
    "    # Step 1: Select n0 landmarks uniformly at random from the data set X\n",
    "    ind0 = np.random.choice(n, n0, replace=False)\n",
    "    Z1 = X[ind0]  # Z1 represents the uniformly selected landmarks\n",
    "\n",
    "    # Step 2: Use importance sampling to select n1 landmarks based on a distribution proportional to importance scores\n",
    "    ind_remain = np.delete(np.arange(n), ind0)\n",
    "    X_sub = X[ind_remain]  # Subsampled data set without the previously selected landmarks\n",
    "    Dist = euclidean_distances(X_sub, Z1, squared=False) / sigma  # Compute distances between points and landmarks\n",
    "    DistMin = Dist.min(axis=1)  # Minimum distance to any landmark for each point\n",
    "    Prob = (1 - omega) / len(ind_remain) + omega * (DistMin / np.sum(DistMin))  # Importance scores for data points\n",
    "    ind1 = np.random.choice(len(X_sub), size=n_important, replace=False, p=Prob)  # Sample n1 points based on scores\n",
    "    Z2 = KMeans(n_clusters=n1, max_iter=100).fit(X_sub[ind1]).cluster_centers_  # Apply K-means to get n1 cluster centroids\n",
    "\n",
    "    # Concatenate the two sets of landmarks to get the final set\n",
    "    Z = np.concatenate((Z1, Z2), axis=0)\n",
    "\n",
    "    return Z  # Return the final set of importance-sampled landmarks\n",
    "\n",
    "def nystrom(X, Z, sigma, r):\n",
    "    '''Computing low-rank approximation using the Nystrom method'''\n",
    "    # Compute the Nystrom low-rank approximation of the kernel matrix for given data points X and landmarks Z\n",
    "    C = np.exp(-euclidean_distances(X, Z, squared=True) / (sigma ** 2))  # Kernel matrix between X and Z\n",
    "    W = np.exp(-euclidean_distances(Z, squared=True) / (sigma ** 2))  # Kernel matrix among the landmarks Z\n",
    "    W = (W + W.T) / 2  # Symmetrize the kernel matrix W\n",
    "    Q, R = la.qr(C, mode='reduced')  # Compute the reduced QR decomposition of the kernel matrix C\n",
    "    V, Sigma, _ = la.svd(la.multi_dot([R, np.linalg.pinv(W), np.transpose(R)]))  # Compute SVD for Nystrom approximation\n",
    "    EigVecNys = np.matmul(Q, V[:, :r])  # Nystrom low-rank eigenvectors\n",
    "    EigValNys = Sigma[:r]  # Nystrom low-rank eigenvalues\n",
    "    return EigVecNys, EigValNys  # Return the Nystrom low-rank approximation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
